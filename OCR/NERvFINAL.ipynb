{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f65602-0315-4af8-b3cc-2a8e7b7baba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 'annotated70data.json' to SpaCy format at 'train_spacy.json'.\n",
      "✅ Converted 'annotated30data.json' to SpaCy format at 'test_spacy.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_labelstudio_to_spacy(label_studio_file, spacy_output_file):\n",
    "    with open(label_studio_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    spacy_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        text = entry[\"data\"][\"text\"]\n",
    "        entities = []\n",
    "\n",
    "        for annotation in entry[\"annotations\"]:\n",
    "            for result in annotation[\"result\"]:\n",
    "                value = result[\"value\"]\n",
    "                start = value[\"start\"]\n",
    "                end = value[\"end\"]\n",
    "                label = value[\"labels\"][0]\n",
    "                entities.append((start, end, label))\n",
    "        \n",
    "        spacy_data.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "\n",
    "    with open(spacy_output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(spacy_data, file, indent=4)\n",
    "    \n",
    "    print(f\"✅ Converted '{label_studio_file}' to SpaCy format at '{spacy_output_file}'.\")\n",
    "\n",
    "# ✅ Convert your training and testing data\n",
    "convert_labelstudio_to_spacy(\"annotated70data.json\", \"train_spacy.json\")\n",
    "convert_labelstudio_to_spacy(\"annotated30data.json\", \"test_spacy.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8028e99c-8068-4e1e-8b80-b55745b45f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azrie_g78nuu0\\anaconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# from spacy.tokens import DocBin\n",
    "# from spacy.training.example import Example\n",
    "# from spacy.training import Example\n",
    "\n",
    "# # Load blank English pipeline\n",
    "# nlp = spacy.blank(\"en\")\n",
    "\n",
    "# # Add RoBERTa transformer and NER\n",
    "# nlp.add_pipe(\"transformer\", config={\"model\": {\"name\": \"roberta-base\"}})\n",
    "# ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# # Add your 14 custom entities\n",
    "# labels = [\n",
    "#     \"INVOICE_NUM\", \"INVOICE_DATE\", \"DUE_DATE\", \"SENDER\", \"EMAIL\", \"PHONE_NUM\",\n",
    "#     \"ITEM_NAME\", \"PRICE\", \"QUANTITY\", \"TOTAL_PRICE\", \"TOTAL_INVOICE_PRICE\",\n",
    "#     \"BANK_NAME\", \"ACCOUNT_NUM\", \"WEBSITE\"\n",
    "# ]\n",
    "# for label in labels:\n",
    "#     ner.add_label(label)\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Load a blank English pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add a CNN-based tok2vec (default in spaCy for CNN models)\n",
    "tok2vec = nlp.add_pipe(\"tok2vec\")  # CNN-based context encoder\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add your 14 custom entities\n",
    "labels = [\n",
    "    \"INVOICE_NUM\", \"INVOICE_DATE\", \"DUE_DATE\", \"SENDER\", \"EMAIL\", \"PHONE_NUM\",\n",
    "    \"ITEM_NAME\", \"PRICE\", \"QUANTITY\", \"TOTAL_PRICE\", \"TOTAL_INVOICE_PRICE\",\n",
    "    \"BANK_NAME\", \"ACCOUNT_NUM\", \"WEBSITE\"\n",
    "]\n",
    "for label in labels:\n",
    "    ner.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5263c572-80ea-4c1f-870c-e9dffd94001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load converted training data\n",
    "def load_spacy_data(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_data_raw = load_spacy_data(\"train_spacy.json\")\n",
    "test_data_raw = load_spacy_data(\"test_spacy.json\")\n",
    "\n",
    "# Convert to SpaCy Example objects\n",
    "train_examples = []\n",
    "for entry in train_data_raw:\n",
    "    doc = nlp.make_doc(entry[\"text\"])\n",
    "    entities = {\"entities\": [(start, end, label) for start, end, label in entry[\"entities\"]]}\n",
    "    example = Example.from_dict(doc, entities)\n",
    "    train_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782010d6-96e6-42a6-a251-c0526fe9a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3624.0894\n",
      "Epoch 2, Loss: 1517.2960\n",
      "Epoch 3, Loss: 1354.9921\n",
      "Epoch 4, Loss: 1122.9745\n",
      "Epoch 5, Loss: 1185.2572\n",
      "Epoch 6, Loss: 890.2052\n",
      "Epoch 7, Loss: 836.7438\n",
      "Epoch 8, Loss: 779.1541\n",
      "Epoch 9, Loss: 807.8812\n",
      "Epoch 10, Loss: 583.0440\n",
      "Epoch 11, Loss: 576.5822\n",
      "Epoch 12, Loss: 542.2830\n",
      "Epoch 13, Loss: 469.3046\n",
      "Epoch 14, Loss: 386.7404\n",
      "Epoch 15, Loss: 377.9493\n",
      "Epoch 16, Loss: 435.4324\n",
      "Epoch 17, Loss: 412.9754\n",
      "Epoch 18, Loss: 325.0068\n",
      "Epoch 19, Loss: 307.3699\n",
      "Epoch 20, Loss: 287.0491\n",
      "Epoch 21, Loss: 282.0897\n",
      "Epoch 22, Loss: 217.6070\n",
      "Epoch 23, Loss: 292.1536\n",
      "Epoch 24, Loss: 221.1254\n",
      "Epoch 25, Loss: 211.3847\n",
      "Epoch 26, Loss: 255.4597\n",
      "Epoch 27, Loss: 228.6534\n",
      "Epoch 28, Loss: 180.8164\n",
      "Epoch 29, Loss: 165.0786\n",
      "Epoch 30, Loss: 188.2148\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import compounding, minibatch\n",
    "\n",
    "\n",
    "# Initialize components with training examples\n",
    "optimizer = nlp.initialize(get_examples=lambda: train_examples)\n",
    "\n",
    "# Early stopping config\n",
    "best_loss = float(\"inf\")\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "no_improvement = 0\n",
    "max_epochs = 30\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    random.shuffle(train_examples)\n",
    "    losses = {}\n",
    "\n",
    "    # Use smaller batches with slower growth to simulate Code A’s exposure\n",
    "    batches = minibatch(train_examples, size=compounding(2.0, 16.0, 1.001))\n",
    "    \n",
    "    for batch in batches:\n",
    "        nlp.update(batch, drop=0.3, losses=losses)\n",
    "\n",
    "    current_loss = losses.get(\"ner\", 0.0)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if best_loss - current_loss > min_delta:\n",
    "        best_loss = current_loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"⏹️ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5dfedc9-ce8a-45a2-a147-749d27b7b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"ner_cnn_30e\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
